Automatically generated by Mendeley Desktop 1.11
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@book{Rumelhart1986,
author = {Rumelhart, DE and Hinton, Geoffrey E. and Williams, RJ},
file = {:Users/milanlajtos/Documents/Mendeley Desktop/Rumelhart, Hinton, Williams - 1988 - Learning representations by back-propagating errors.pdf:pdf},
title = {{Learning representations by back-propagating errors}},
url = {http://www.cs.toronto.edu/~hinton/absps/naturebp.pdf},
year = {1988}
}
@article{Laughlin2003,
abstract = {Brains perform with remarkable efficiency, are capable of prodigious computation, and are marvels of communication. We are beginning to understand some of the geometric, biophysical, and energy constraints that have governed the evolution of cortical networks. To operate efficiently within these constraints, nature has optimized the structure and function of cortical networks with design principles similar to those used in electronic networks. The brain also exploits the adaptability of biological systems to reconfigure in response to changing needs.},
author = {Laughlin, Simon B and Sejnowski, Terrence J},
doi = {10.1126/science.1089662},
file = {:Users/milanlajtos/Documents/Mendeley Desktop/Laughlin, Sejnowski - 2003 - Communication in neuronal networks.pdf:pdf},
issn = {1095-9203},
journal = {Science (New York, N.Y.)},
keywords = {Action Potentials,Animals,Biological Evolution,Brain,Brain: physiology,Cell Communication,Humans,Nerve Net,Nerve Net: physiology,Neuronal Plasticity,Neurons,Neurons: physiology,Synaptic Transmission},
month = sep,
number = {5641},
pages = {1870--4},
pmid = {14512617},
title = {{Communication in neuronal networks.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2930149\&tool=pmcentrez\&rendertype=abstract},
volume = {301},
year = {2003}
}
@article{Turing1936,
author = {Turing, Alan M.},
file = {:Users/milanlajtos/Documents/Mendeley Desktop/Turing - 1936 - On Computable Numbers with an Application to the Entscheidungsproblem.pdf:pdf},
journal = {J. of Math 58 (1936): 345-363.},
pages = {345--363},
title = {{On Computable Numbers with an Application to the Entscheidungsproblem}},
url = {http://classes.soe.ucsc.edu/cmps210/Winter11/Papers/turing-1936.pdf},
volume = {58},
year = {1936}
}
@article{Bengio1994,
author = {Bengio, Yoshua and Simard, P and Frasconi, Paolo},
file = {:Users/milanlajtos/Documents/Mendeley Desktop/Bengio, Simard, Frasconi - 1994 - Learning Long-Term Dependencies with Gradient Descent is Difficult.pdf:pdf},
journal = {Neural Networks, IEEE},
title = {{Learning Long-Term Dependencies with Gradient Descent is Difficult}},
url = {http://www-dsi.ing.unifi.it/~paolo/ps/tnn-94-gradient.pdf},
year = {1994}
}
@article{Jia2013,
archivePrefix = {arXiv},
arxivId = {arXiv:1310.1531v1},
author = {Donahue, Jeff and Jia, Yangqing and Vinyals, Oriol and Hoffman, Judy and Zhang, Ning and Tzeng, Eric and Darrell, Trevor and Eecs, Trevor and Edu, Berkeley},
eprint = {arXiv:1310.1531v1},
file = {:Users/milanlajtos/Documents/Mendeley Desktop/Donahue et al. - Unknown - DeCAF A Deep Convolutional Activation Feature for Generic Visual Recognition.pdf:pdf},
title = {{DeCAF : A Deep Convolutional Activation Feature for Generic Visual Recognition}},
url = {http://arxiv.org/pdf/1310.1531v1.pdf}
}
@electronic{Victor2011,
author = {Victor, Bret},
keywords = {abstraction,visualization},
title = {{Up and Down the Ladder of Abstraction}},
url = {http://worrydream.com/LadderOfAbstraction/},
year = {2011}
}
@book{Bruner1966,
author = {Bruner, J S},
publisher = {Belknap Press of Harvard University Press},
series = {Books that live},
title = {{Toward a Theory of Instruction}},
url = {https://encrypted.google.com/books?id=xR44nQEACAAJ},
year = {1966}
}
@incollection{Turing1948,
address = {Edinburgh, UK},
annote = {Hodges page 377 note 6.53.},
author = {Turing, Alan M.},
booktitle = {Machine Intelligence},
chapter = {1},
editor = {Meltzer, Bernard and Michie, Donald},
keywords = {algorithms,genetic programming,robot},
pages = {3--23},
publisher = {Edinburgh University Press},
title = {{Intelligent Machinery}},
volume = {5},
year = {1969}
}
@article{Nair2010,
author = {Nair, Vinod and Hinton, Geoffrey E.},
file = {:Users/milanlajtos/Documents/Mendeley Desktop/Nair, Hinton - 2010 - Rectified Linear Units Improve Restricted Boltzmann Machines.pdf:pdf},
journal = {International Conference on Machine Learning (ICML-10)},
number = {3},
title = {{Rectified Linear Units Improve Restricted Boltzmann Machines}},
url = {http://machinelearning.wustl.edu/mlpapers/paper\_files/icml2010\_NairH10.pdf},
year = {2010}
}
@article{Salakhutdinov2009a,
author = {Salakhutdinov, Ruslan R. and Hinton, Geoffrey E.},
doi = {10.1016/j.ijar.2008.11.006},
file = {:Users/milanlajtos/Documents/Mendeley Desktop/Salakhutdinov, Hinton - 2009 - Semantic hashing.pdf:pdf},
issn = {0888613X},
journal = {International Journal of Approximate Reasoning},
month = jul,
number = {7},
pages = {969--978},
title = {{Semantic hashing}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0888613X08001813},
volume = {50},
year = {2009}
}
@article{Hinton2007,
abstract = {To achieve its impressive performance in tasks such as speech perception or object recognition, the brain extracts multiple levels of representation from the sensory input. Backpropagation was the first computationally efficient model of how neural networks could learn multiple layers of representation, but it required labeled training data and it did not work well in deep networks. The limitations of backpropagation learning can now be overcome by using multilayer neural networks that contain top-down connections and training them to generate sensory data rather than to classify it. Learning multilayer generative models might seem difficult, but a recent discovery makes it easy to learn nonlinear distributed representations one layer at a time.},
author = {Hinton, Geoffrey E.},
doi = {10.1016/j.tics.2007.09.004},
file = {:Users/milanlajtos/Documents/Mendeley Desktop/Hinton - 2007 - Learning Multiple Layers of Representations.pdf:pdf},
issn = {1364-6613},
journal = {Trends in cognitive sciences},
keywords = {Brain,Brain: physiology,Humans,Learning,Learning: physiology,Models,Nerve Net,Nerve Net: physiology,Psychological},
month = oct,
number = {10},
pages = {428--34},
pmid = {17921042},
title = {{Learning Multiple Layers of Representations}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17921042},
volume = {11},
year = {2007}
}
@article{Barabasi2009,
abstract = {For decades, we tacitly assumed that the components of such complex systems as the cell, the society, or the Internet are randomly wired together. In the past decade, an avalanche of research has shown that many real networks, independent of their age, function, and scope, converge to similar architectures, a universality that allowed researchers from different disciplines to embrace network theory as a common paradigm. The decade-old discovery of scale-free networks was one of those events that had helped catalyze the emergence of network science, a new research field with its distinct set of challenges and accomplishments.},
author = {Barab\'{a}si, Albert-L\'{a}szl\'{o}},
doi = {10.1126/science.1173299},
file = {:Users/milanlajtos/Documents/Mendeley Desktop/Barab\'{a}si - 2009 - Scale-free networks a decade and beyond.pdf:pdf},
issn = {1095-9203},
journal = {Science (New York, N.Y.)},
keywords = {Models, Theoretical,Systems Theory},
month = jul,
number = {5939},
pages = {412--3},
pmid = {19628854},
title = {{Scale-free networks: a decade and beyond.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/19628854},
volume = {325},
year = {2009}
}
@article{Le2012,
author = {Le, Quoc V and Ranzato, Marc' Aurelio and Devin, Matthieu and Corrado, Greg S and Ng, Andrew Y.},
file = {:Users/milanlajtos/Documents/Mendeley Desktop/Le et al. - 2012 - Building High-level Features Using Large Scale Unsupervised Learning.pdf:pdf},
journal = {Acoustics, Speech and Signal Processing (ICASSP)},
title = {{Building High-level Features Using Large Scale Unsupervised Learning}},
url = {http://static.googleusercontent.com/media/research.google.com/en//archive/unsupervised\_icml2012.pdf},
year = {2012}
}
@book{Rojas96,
address = {Berlin},
author = {Rojas, Raul},
file = {:Users/milanlajtos/Documents/Mendeley Desktop//Rojas - 1996 - Neural Networks A Systematic Introduction.pdf:pdf},
keywords = {SOM clustering ebook neural-networks seminar},
publisher = {Springer-Verlag},
title = {{Neural Networks: A Systematic Introduction}},
url = {http://www.inf.fu-berlin.de/inst/ag-ki/rojas\_home/pmwiki/pmwiki.php?n=Books.NeuralNetworksBook},
year = {1996}
}
@article{Pape2011,
author = {Pape, Leo and Gomez, Faustino and Ring, Mark and Schmidhuber, Jurgen},
doi = {10.1109/IJCNN.2011.6033359},
file = {:Users/milanlajtos/Documents/Mendeley Desktop/Pape et al. - 2011 - Modular deep belief networks that do not forget.pdf:pdf},
isbn = {978-1-4244-9635-8},
journal = {The 2011 International Joint Conference on Neural Networks},
month = jul,
pages = {1191--1198},
publisher = {Ieee},
title = {{Modular deep belief networks that do not forget}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6033359},
year = {2011}
}
@inproceedings{Beran2008,
author = {Beran, Peter Paul and Vinek, Elisabeth and Schikuta, Erich and Weish\"{a}upl, Thomas},
booktitle = {IJCNN},
keywords = {dblp},
pages = {1872--1879},
publisher = {IEEE},
title = {{ViNNSL - the Vienna Neural Network Specification Language.}},
url = {http://dblp.uni-trier.de/db/conf/ijcnn/ijcnn2008.html\#BeranVSW08},
year = {2008}
}
@article{Beste2013,
abstract = {Achieving high-level skills is generally considered to require intense training, which is thought to optimally engage neuronal plasticity mechanisms. Recent work, however, suggests that intensive training may not be necessary for skill learning. Skills can be effectively acquired by a complementary approach in which the learning occurs in response to mere exposure to repetitive sensory stimulation. Such training-independent sensory learning induces lasting changes in perception and goal-directed behaviour in humans, without any explicit task training. We suggest that the effectiveness of this form of learning in different sensory domains stems from the fact that the stimulation protocols used are optimized to alter synaptic transmission and efficacy. While this approach directly links behavioural research in humans with studies on cellular plasticity, other approaches show that learning can occur even in the absence of an actual stimulus. These include learning through imagery or feedback-induced cortical activation, resulting in learning without task training. All these approaches challenge our understanding of the mechanisms that mediate learning. Apparently, humans can learn under conditions thought to be impossible a few years ago. Although the underlying mechanisms are far from being understood, training-independent sensory learning opens novel possibilities for applications aimed at augmenting human cognition.},
author = {Beste, Christian and Dinse, Hubert R},
doi = {10.1016/j.cub.2013.04.044},
file = {:Users/milanlajtos/Documents/Mendeley Desktop/Beste, Dinse - 2013 - Learning without training.pdf:pdf},
issn = {1879-0445},
journal = {Current biology : CB},
keywords = {Brain,Brain: physiology,Cognition,Goals,Humans,Learning,Long-Term Potentiation,Long-Term Synaptic Depression,Motor Skills,Neuronal Plasticity,Perception,Sensation,Sensory Thresholds},
month = jun,
number = {11},
pages = {R489--99},
pmid = {23743417},
publisher = {Elsevier Ltd},
title = {{Learning without training.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23743417},
volume = {23},
year = {2013}
}
@book{Petru2007,
address = {Praha},
author = {Petrů, Marek},
edition = {Filosofie},
isbn = {978-80-7254-969-6},
pages = {385},
publisher = {Triton},
title = {{Fyziologie mysli: \'{U}vod do kognitivn\'{\i} v\v{e}dy}},
year = {2007}
}
@article{Hubel1968,
author = {Hubel, DH and Wiesel, TN},
file = {:Users/milanlajtos/Documents/Mendeley Desktop/Hubel, Wiesel - 1968 - Receptive fields and functional architecture of monkey striate cortex.pdf:pdf},
journal = {The Journal of physiology},
pages = {215--243},
title = {{Receptive fields and functional architecture of monkey striate cortex}},
url = {http://jp.physoc.org/content/195/1/215.short},
year = {1968}
}
@article{Hinton2011,
author = {Hinton, Geoffrey E. and Salakhutdinov, Ruslan R.},
doi = {10.1111/j.1756-8765.2010.01109.x},
file = {:Users/milanlajtos/Documents/Mendeley Desktop/Hinton, Salakhutdinov - 2011 - Discovering Binary Codes for Documents by Learning Deep Generative Models.pdf:pdf},
issn = {17568757},
journal = {Topics in Cognitive Science},
keywords = {auto-encoders,binary codes,deep learning,document retrieval,restricted boltzmann machines,semantic hashing},
month = jan,
number = {1},
pages = {74--91},
title = {{Discovering Binary Codes for Documents by Learning Deep Generative Models}},
url = {http://doi.wiley.com/10.1111/j.1756-8765.2010.01109.x},
volume = {3},
year = {2011}
}
@book{Bengio2009,
author = {Bengio, Yoshua},
booktitle = {Foundations and Trends® in Machine Learning},
doi = {10.1561/2200000006},
file = {:Users/milanlajtos/Documents/Mendeley Desktop/Bengio - 2009 - Learning Deep Architectures for AI.pdf:pdf},
isbn = {2200000006},
issn = {1935-8237},
number = {1},
pages = {1--127},
title = {{Learning Deep Architectures for AI}},
url = {http://www.nowpublishers.com/product.aspx?product=MAL\&doi=2200000006},
volume = {2},
year = {2009}
}
@article{Hinton2006a,
abstract = {High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such "autoencoder" networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.},
annote = {        From Duplicate 2 (                           Reducing the dimensionality of data with neural networks.                         - Hinton, G E; Salakhutdinov, R R )
                
        
        
      },
author = {Hinton, Geoffrey E. and Salakhutdinov, Ruslan R.},
doi = {10.1126/science.1127647},
file = {:Users/milanlajtos/Documents/Mendeley Desktop/Hinton, Salakhutdinov - 2006 - Reducing the dimensionality of data with neural networks(2).pdf:pdf;:Users/milanlajtos/Documents/Mendeley Desktop/Hinton, Salakhutdinov - 2006 - Reducing the dimensionality of data with neural networks.pdf:pdf},
issn = {1095-9203},
journal = {Science (New York, N.Y.)},
month = jul,
number = {5786},
pages = {504--7},
pmid = {16873662},
title = {{Reducing the dimensionality of data with neural networks.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/16873662},
volume = {313},
year = {2006}
}
@article{Cybenko1989,
author = {Cybenko, George},
file = {:Users/milanlajtos/Documents/Mendeley Desktop/Cybenko - 1989 - Approximation by Superpositions of a Sigmoidal Function.pdf:pdf},
journal = {Mathematics of control, signals and systems},
keywords = {approximation,completeness,neural networks},
pages = {303--314},
title = {{Approximation by Superpositions of a Sigmoidal Function}},
url = {http://deeplearning.cs.cmu.edu/pdfs/Cybenko.pdf},
year = {1989}
}
@article{Krizhevsky2012,
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
file = {:Users/milanlajtos/Documents/Mendeley Desktop/Krizhevsky, Sutskever, Hinton - 2012 - ImageNet Classification with Deep Convolutional Neural Networks.pdf:pdf},
journal = {NIPS},
pages = {1--9},
title = {{ImageNet Classification with Deep Convolutional Neural Networks.}},
url = {https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf},
year = {2012}
}
@article{Schmidhuber2014,
abstract = {In recent years, deep neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarises relevant work, much of it from the previous millennium. Shallow and deep learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning \& evolutionary computation, and indirect search for short programs encoding deep and large networks.},
archivePrefix = {arXiv},
arxivId = {1404.7828},
author = {Schmidhuber, Juergen},
eprint = {1404.7828},
month = apr,
pages = {66},
title = {{Deep Learning in Neural Networks: An Overview}},
url = {http://arxiv.org/abs/1404.7828},
year = {2014}
}
@book{Hobbes2009,
address = {Praha},
author = {Hobbes, Thomas},
edition = {Knihovna n},
file = {:Users/milanlajtos/Documents/Mendeley Desktop/Hobbes - 2009 - Leviathan neboli l\'{a}tka, forma a moc st\'{a}tu c\'{\i}rkevn\'{\i}ho a ob\v{c}ansk\'{e}ho.pdf:pdf},
pages = {513 s.},
publisher = {OIKOYMENH},
title = {{Leviathan neboli l\'{a}tka, forma a moc st\'{a}tu c\'{\i}rkevn\'{\i}ho a ob\v{c}ansk\'{e}ho}},
year = {2009}
}
@article{Arnold2011,
author = {Arnold, Ludovic and Rebecchi, S\'{e}bastien and Chevallier, Sylvain and Paugam-Moisy, H},
file = {:Users/milanlajtos/Documents/Mendeley Desktop/Arnold et al. - 2011 - An Introduction to Deep Learning.pdf:pdf},
journal = {ESANN},
title = {{An Introduction to Deep Learning.}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?rep=rep1\&type=pdf\&doi=10.1.1.227.1221},
year = {2011}
}
@article{Blei2010,
author = {Blei, David and Carin, Lawrence and Dunson, David},
doi = {10.1109/MSP.2010.938079},
file = {:Users/milanlajtos/Documents/Mendeley Desktop/Blei, Carin, Dunson - 2010 - Probabilistic Topic Models.pdf:pdf},
issn = {1053-5888},
journal = {IEEE Signal Processing Magazine},
month = nov,
pages = {77--84},
title = {{Probabilistic Topic Models}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5563111},
year = {2010}
}
@article{Memisevic2010,
abstract = {To allow the hidden units of a restricted Boltzmann machine to model the transformation between two successive images, Memisevic and Hinton (2007) introduced three-way multiplicative interactions that use the intensity of a pixel in the first image as a multiplicative gain on a learned, symmetric weight between a pixel in the second image and a hidden unit. This creates cubically many parameters, which form a three-dimensional interaction tensor. We describe a low-rank approximation to this interaction tensor that uses a sum of factors, each of which is a three-way outer product. This approximation allows efficient learning of transformations between larger image patches. Since each factor can be viewed as an image filter, the model as a whole learns optimal filter pairs for efficiently representing transformations. We demonstrate the learning of optimal filter pairs from various synthetic and real image sequences. We also show how learning about image transformations allows the model to perform a simple visual analogy task, and we show how a completely unsupervised network trained on transformations perceives multiple motions of transparent dot patterns in the same way as humans.},
author = {Memisevic, Roland and Hinton, Geoffrey E.},
doi = {10.1162/neco.2010.01-09-953},
file = {:Users/milanlajtos/Documents/Mendeley Desktop/Memisevic, Hinton - 2010 - Learning to represent spatial transformations with factored higher-order Boltzmann machines.pdf:pdf},
issn = {1530-888X},
journal = {Neural computation},
keywords = {Algorithms,Artificial Intelligence,Image Processing, Computer-Assisted,Image Processing, Computer-Assisted: methods,Mathematical Concepts,Neural Networks (Computer),Pattern Recognition, Automated,Pattern Recognition, Automated: methods,Pattern Recognition, Visual,Pattern Recognition, Visual: physiology,Space Perception,Space Perception: physiology},
month = jun,
number = {6},
pages = {1473--92},
pmid = {20141471},
title = {{Learning to represent spatial transformations with factored higher-order Boltzmann machines.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/20141471},
volume = {22},
year = {2010}
}
@article{LeCunn1998,
author = {LeCun, Yann and Bottou, L\'{e}on and Bengio, Yoshua and Patrick, Haffner},
file = {:Users/milanlajtos/Documents/Mendeley Desktop/LeCun et al. - 1998 - Gradient-based learning applied to document recognition.pdf:pdf},
journal = {Proceedings of the IEEE},
title = {{Gradient-based learning applied to document recognition}},
url = {http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf},
year = {1998}
}
@article{Caze2013,
abstract = {Local supra-linear summation of excitatory inputs occurring in pyramidal cell dendrites, the so-called dendritic spikes, results in independent spiking dendritic sub-units, which turn pyramidal neurons into two-layer neural networks capable of computing linearly non-separable functions, such as the exclusive OR. Other neuron classes, such as interneurons, may possess only a few independent dendritic sub-units, or only passive dendrites where input summation is purely sub-linear, and where dendritic sub-units are only saturating. To determine if such neurons can also compute linearly non-separable functions, we enumerate, for a given parameter range, the Boolean functions implementable by a binary neuron model with a linear sub-unit and either a single spiking or a saturating dendritic sub-unit. We then analytically generalize these numerical results to an arbitrary number of non-linear sub-units. First, we show that a single non-linear dendritic sub-unit, in addition to the somatic non-linearity, is sufficient to compute linearly non-separable functions. Second, we analytically prove that, with a sufficient number of saturating dendritic sub-units, a neuron can compute all functions computable with purely excitatory inputs. Third, we show that these linearly non-separable functions can be implemented with at least two strategies: one where a dendritic sub-unit is sufficient to trigger a somatic spike; another where somatic spiking requires the cooperation of multiple dendritic sub-units. We formally prove that implementing the latter architecture is possible with both types of dendritic sub-units whereas the former is only possible with spiking dendrites. Finally, we show how linearly non-separable functions can be computed by a generic two-compartment biophysical model and a realistic neuron model of the cerebellar stellate cell interneuron. Taken together our results demonstrate that passive dendrites are sufficient to enable neurons to compute linearly non-separable functions.},
author = {Caz\'{e}, Romain Daniel and Humphries, Mark and Gutkin, Boris},
doi = {10.1371/journal.pcbi.1002867},
file = {:Users/milanlajtos/Documents/Mendeley Desktop/Caz\'{e}, Humphries, Gutkin - 2013 - Passive dendrites enable single neurons to compute linearly non-separable functions.pdf:pdf},
issn = {1553-7358},
journal = {PLoS computational biology},
keywords = {Action Potentials,Action Potentials: physiology,Animals,Cerebellum,Cerebellum: cytology,Cerebellum: physiology,Computational Biology,Computational Biology: methods,Dendrites,Dendrites: physiology,Linear Models,Mice,Models,Neurological,Neurons,Neurons: physiology},
month = jan,
number = {2},
pages = {e1002867},
pmid = {23468600},
title = {{Passive dendrites enable single neurons to compute linearly non-separable functions.}},
url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3585427/},
volume = {9},
year = {2013}
}
@article{Hinton2012,
archivePrefix = {arXiv},
arxivId = {arXiv:1207.0580v1},
author = {Hinton, Geoffrey E. and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R.},
eprint = {arXiv:1207.0580v1},
file = {:Users/milanlajtos/Documents/Mendeley Desktop/Hinton et al. - 2012 - Improving neural networks by preventing co-adaptation of feature detectors.pdf:pdf},
journal = {arXiv preprint arXiv: \ldots},
keywords = {Hinton2012},
pages = {1--18},
title = {{Improving neural networks by preventing co-adaptation of feature detectors}},
url = {http://arxiv.org/abs/1207.0580},
year = {2012}
}
@article{Nitsche2008,
abstract = {Effects of weak electrical currents on brain and neuronal function were first described decades ago. Recently, DC polarization of the brain was reintroduced as a noninvasive technique to alter cortical activity in humans. Beyond this, transcranial direct current stimulation (tDCS) of different cortical areas has been shown, in various studies, to result in modifications of perceptual, cognitive, and behavioral functions. Moreover, preliminary data suggest that it can induce beneficial effects in brain disorders. Brain stimulation with weak direct currents is a promising tool in human neuroscience and neurobehavioral research. To facilitate and standardize future tDCS studies, we offer this overview of the state of the art for tDCS.},
author = {Nitsche, Michael a and Cohen, Leonardo G and Wassermann, Eric M and Priori, Alberto and Lang, Nicolas and Antal, Andrea and Paulus, Walter and Hummel, Friedhelm and Boggio, Paulo S and Fregni, Felipe and Pascual-Leone, Alvaro},
doi = {10.1016/j.brs.2008.06.004},
file = {:Users/milanlajtos/Documents/Mendeley Desktop/Nitsche et al. - 2008 - Transcranial direct current stimulation State of the art 2008.pdf:pdf},
issn = {1935-861X},
journal = {Brain stimulation},
keywords = {Animals,Cerebral Cortex,Cerebral Cortex: physiology,Humans,Research Design,Transcranial Magnetic Stimulation,Transcranial Magnetic Stimulation: instrumentation,Transcranial Magnetic Stimulation: methods,Transcutaneous Electric Nerve Stimulation,Transcutaneous Electric Nerve Stimulation: instrum,Transcutaneous Electric Nerve Stimulation: methods},
month = jul,
number = {3},
pages = {206--23},
pmid = {20633386},
title = {{Transcranial direct current stimulation: State of the art 2008.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/20633386},
volume = {1},
year = {2008}
}
@electronic{Victor2013,
author = {Victor, Bret},
title = {{Media for Thinking the Unthinkable}},
url = {http://worrydream.com/MediaForThinkingTheUnthinkable/},
year = {2013}
}
@article{Glorot2011,
author = {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
file = {:Users/milanlajtos/Documents/Mendeley Desktop/Glorot, Bordes, Bengio - 2011 - Deep Sparse Rectifier Neural Networks.pdf:pdf},
journal = {Proceedings of the 14th International Conference on Artificial Intelligence and Statistics. JMLR W\&CP},
pages = {315--323},
title = {{Deep Sparse Rectifier Neural Networks}},
url = {http://eprints.pascal-network.org/archive/00008596/01/glorot11a.pdf},
volume = {15},
year = {2011}
}
@article{Ranzato2008,
address = {New York, New York, USA},
author = {Ranzato, Marc' Aurelio and Szummer, Martin},
doi = {10.1145/1390156.1390256},
file = {:Users/milanlajtos/Documents/Mendeley Desktop/Ranzato, Szummer - 2008 - Semi-supervised learning of compact document representations with deep networks.pdf:pdf},
isbn = {9781605582054},
journal = {Proceedings of the 25th international conference on Machine learning - ICML '08},
pages = {792--799},
publisher = {ACM Press},
title = {{Semi-supervised learning of compact document representations with deep networks}},
url = {http://portal.acm.org/citation.cfm?doid=1390156.1390256},
year = {2008}
}
@article{Maas2013,
author = {Maas, Andrew L. and Hannun, Awni Y. and Ng, Andrew Y.},
file = {:Users/milanlajtos/Documents/Mendeley Desktop/Maas, Hannun, Ng - 2013 - Rectifier Nonlinearities Improve Neural Network Acoustic Models.pdf:pdf},
journal = {Proceedings of the ICML},
title = {{Rectifier Nonlinearities Improve Neural Network Acoustic Models}},
url = {http://ai.stanford.edu/~amaas/papers/relu\_hybrid\_icml2013\_final.pdf},
volume = {28},
year = {2013}
}
@article{Teuscher2001,
author = {Teuscher, Christof and Sanchez, Eduardo},
file = {:Users/milanlajtos/Documents/Mendeley Desktop//Teuscher, Sanchez - 2001 - A revival of Turing's forgotten connectionist ideas exploring unorganized machines.pdf:pdf;:Users/milanlajtos/Documents/Mendeley Desktop//Teuscher, Sanchez - 2001 - A revival of Turing's forgotten connectionist ideas exploring unorganized machines.pdf:pdf},
journal = {Connectionist Models of Learning, Development \ldots},
title = {{A revival of Turing's forgotten connectionist ideas: exploring unorganized machines}},
url = {http://link.springer.com/chapter/10.1007/978-1-4471-0281-6\_16},
year = {2001}
}
@book{Johnson2010,
address = {San Francisco, CA, USA},
author = {Johnson, Jeff},
isbn = {012375030X, 9780123750303},
publisher = {Morgan Kaufmann Publishers Inc.},
title = {{Designing with the Mind in Mind: Simple Guide to Understanding User Interface Design Rules}},
year = {2010}
}
@article{McCulloch1943,
author = {McCulloch, WS and Pitts, W},
file = {:Users/milanlajtos/Documents/Mendeley Desktop/McCulloch, Pitts - 1943 - A logical calculus of the ideas immanent in nervous activity.pdf:pdf},
journal = {The bulletin of mathematical biophysics},
pages = {115--133},
title = {{A logical calculus of the ideas immanent in nervous activity}},
url = {http://link.springer.com/article/10.1007/BF02478259},
volume = {5},
year = {1943}
}
@article{Zeiler2013,
archivePrefix = {arXiv},
arxivId = {arXiv:1311.2901v3},
author = {Zeiler, MD and Fergus, Rob},
eprint = {arXiv:1311.2901v3},
file = {:Users/milanlajtos/Documents/Mendeley Desktop/Zeiler, Fergus - 2013 - Visualizing and Understanding Convolutional Neural Networks.pdf:pdf},
journal = {arXiv preprint arXiv:1311.2901},
title = {{Visualizing and Understanding Convolutional Neural Networks}},
url = {http://arxiv.org/abs/1311.2901},
year = {2013}
}
@article{Salakhutdinov2007,
address = {New York, New York, USA},
author = {Salakhutdinov, Ruslan R. and Mnih, Andriy and Hinton, Geoffrey E.},
doi = {10.1145/1273496.1273596},
file = {:Users/milanlajtos/Documents/Mendeley Desktop/Salakhutdinov, Mnih, Hinton - 2007 - Restricted Boltzmann machines for collaborative filtering.pdf:pdf},
isbn = {9781595937933},
journal = {Proceedings of the 24th international conference on Machine learning - ICML '07},
pages = {791--798},
publisher = {ACM Press},
title = {{Restricted Boltzmann machines for collaborative filtering}},
url = {http://portal.acm.org/citation.cfm?doid=1273496.1273596},
year = {2007}
}
@book{Copelanad2004,
address = {New York, New York, USA},
author = {Turing, Alan M. and Copeland, Jack},
file = {:Users/milanlajtos/Documents/Mendeley Desktop/Turing, Copeland - 2004 - The Essential Turing Seminal Writings in Computing, Logic, Philosophy, Artificial Intelligence, and Artificial.pdf:pdf},
isbn = {0-19-825079-7},
publisher = {Oxford University Press Inc.},
title = {{The Essential Turing: Seminal Writings in Computing, Logic, Philosophy, Artificial Intelligence, and Artificial Life, plus The Secrets of Enigma}},
url = {http://ebooks-it.org/e-books/mix/oxford-university/Oxford.University.The.Essential.Turing.Nov.2004.ISBN.0198250800.pdf},
year = {2004}
}
@article{Schmidhuber,
author = {Schmidhuber, Jurgen},
file = {:Users/milanlajtos/Documents/Mendeley Desktop/Schmidhuber - Unknown - A GENERAL METHOD FOR INCREMENTAL SELF-IMPROVEMENT AND MULTI-AGENT LEARNING.pdf:pdf},
journal = {Citeseer},
title = {{A GENERAL METHOD FOR INCREMENTAL SELF-IMPROVEMENT AND MULTI-AGENT LEARNING}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.49.5701\&rep=rep1\&type=pdf}
}
